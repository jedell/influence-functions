{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OPsgGWVXkLUq"
      },
      "outputs": [],
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C-il8r5lyRu",
        "outputId": "a4ea0b4b-98ae-4658-b017-0803028601e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.12)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.12 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.12)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.12->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.12->boto3) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.12->boto3) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets boto3 einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Roj95dszhz01",
        "outputId": "25591074-3bb8-43e8-c1c5-e71bdc81dc03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /root/.passwd-s3fs\n"
          ]
        }
      ],
      "source": [
        "%%writefile ~/.passwd-s3fs\n",
        "# lol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHOjWh3Dicih",
        "outputId": "5a0fa33e-e6b2-4d4a-d224-51a37a5ba238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "s3fs is already the newest version (1.90-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "mkdir: cannot create directory ‘/s3’: File exists\n",
            "s3fs: MOUNTPOINT directory /s3 is not empty. if you are sure this is safe, can use the 'nonempty' mount option.\n",
            "TinyStories_ihvp_1M.pt\tTinyStories_ihvp_tokenwise_33000000.pt\n"
          ]
        }
      ],
      "source": [
        "!chmod 600 ~/.passwd-s3fs\n",
        "!apt install s3fs\n",
        "!mkdir /s3\n",
        "!s3fs baj40ja0abjabucketihvp /s3\n",
        "!ls /s3/ihvp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tyx2IPi1ieAR"
      },
      "outputs": [],
      "source": [
        "!cp /s3/ihvp/TinyStories_ihvp_1M.pt /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rL3Fz4Kklvao"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# https://huggingface.co/datasets/Skylion007/openwebtext\n",
        "from datasets import load_dataset\n",
        "\n",
        "class TinyStoriesDataset(Dataset): # IterableDataset\n",
        "    def __init__(self, split, tokenizer, path='roneneldan/TinyStories', block_size=1024, seed=42, take=10000, skip=0):\n",
        "        self.split = split\n",
        "        self.block_size = block_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset = load_dataset(path, split=split, streaming=False) #.shuffle(seed=seed) #.skip(skip).take(take).with_format('torch')\n",
        "\n",
        "    def __iter__(self):\n",
        "        for item in self.dataset:\n",
        "            text = item['text']\n",
        "            encoding = self.tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True, padding='max_length', truncation=True, max_length=self.block_size)\n",
        "            input_ids = encoding['input_ids'].squeeze()\n",
        "\n",
        "            # Shift the input_ids and attention_mask to the right and pad\n",
        "            labels = input_ids.clone()\n",
        "            labels[:-1] = input_ids[1:]\n",
        "            labels[-1] = self.tokenizer.pad_token_id\n",
        "\n",
        "            yield {'input_ids': input_ids}, {'input_ids': labels}\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.dataset[index]\n",
        "        text = item['text']\n",
        "        encoding = self.tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True, padding='max_length', truncation=True, max_length=self.block_size)\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "\n",
        "        # Shift the input_ids and attention_mask to the right and pad\n",
        "        labels = input_ids.clone()\n",
        "        labels[:-1] = input_ids[1:]\n",
        "        labels[-1] = self.tokenizer.pad_token_id\n",
        "\n",
        "        return {'input_ids': input_ids}, {'input_ids': labels}\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dataset)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.tokenizer.vocab_size\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaUfXox3RvQc",
        "outputId": "1cadf593-fb59-4061-be16-8a0bf8dc4e62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTNeoForCausalLM(\n",
              "  (transformer): GPTNeoModel(\n",
              "    (wte): Embedding(50257, 64)\n",
              "    (wpe): Embedding(2048, 64)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-7): 8 x GPTNeoBlock(\n",
              "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTNeoAttention(\n",
              "          (attention): GPTNeoSelfAttention(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPTNeoMLP(\n",
              "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch import nn\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRIhTSjz-qH9",
        "outputId": "91ca1bff-1e69-4715-fda9-a703cf88b299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformer.h.0.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.0.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n",
            "transformer.h.1.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.1.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n",
            "transformer.h.2.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.2.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n",
            "transformer.h.3.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.3.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n",
            "transformer.h.4.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.4.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n",
            "transformer.h.5.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.5.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n",
            "transformer.h.6.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.6.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n",
            "transformer.h.7.mlp GPTNeoMLP(\n",
            "  (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
            "  (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (act): NewGELUActivation()\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "transformer.h.7.mlp.c_fc Linear(in_features=64, out_features=256, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# 1. get a_l-1, use forward hook to save input to a layer l during the forward pass\n",
        "layer_inputs = {}\n",
        "\n",
        "# TODO something going on with the shapes\n",
        "# ONLY ON INPUT TO MLP (batch_size, seq_len, input_dim)\n",
        "def forward_hook_fn(module, inputs):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        print(inputs[0].shape)\n",
        "        layer_inputs[module] = torch.cat([\n",
        "                inputs[0],\n",
        "                torch.ones((inputs[0].shape[0], inputs[0].shape[1], 1)).to(inputs[0].device),\n",
        "        ], dim=-1).clone().detach()\n",
        "\n",
        "# 2. get grad_loss, gradients of loss wrt output of linear transformation W_l a_l-1\n",
        "#    using a backward hook on the linear layer that saves the gradient wrt the linear layer's output\n",
        "\n",
        "layer_grads = {}\n",
        "\n",
        "# ONLY ON FIRST LINEAR LAYER self.linear(a_l_minus_1)\n",
        "def back_hook_fn(module, grad_input, grad_output):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        layer_grads[module] = grad_output[0].clone().detach()\n",
        "\n",
        "linear_layers = []\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear) and 'mlp' in name: # out_proj ???\n",
        "        # grab linear layers everytime\n",
        "        linear_layers.append(module)\n",
        "\n",
        "linear_layers = linear_layers[:-1]  # remove output token logits layer from calculations\n",
        "\n",
        "for _, module in model.named_modules():\n",
        "    # TODO register miniGPT MLP, first linear layer???\n",
        "    if _.split('.')[-1] == 'mlp':\n",
        "      print(_, module)\n",
        "      # module.register_forward_hook(forward_hook_fn)\n",
        "    if isinstance(module, nn.Linear) and _.split('.')[-1] == 'c_fc':\n",
        "      print(_, module)\n",
        "    #     module.register_full_backward_hook(back_hook_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM499aZT7z1X",
        "outputId": "fc16ba1e-6ed2-4d48-d663-6c9a5d3356b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTNeoForCausalLM(\n",
              "  (transformer): GPTNeoModel(\n",
              "    (wte): Embedding(50257, 64)\n",
              "    (wpe): Embedding(2048, 64)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-7): 8 x GPTNeoBlock(\n",
              "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTNeoAttention(\n",
              "          (attention): GPTNeoSelfAttention(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
              "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPTNeoMLP(\n",
              "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SqkCZ3pZsoIa"
      },
      "outputs": [],
      "source": [
        "def compute_grads(model: nn.Module, train_dataset: DataLoader):\n",
        "\n",
        "    grads = [[] for _ in range(len(linear_layers))]\n",
        "    for X, Y in train_dataset:\n",
        "        model.zero_grad()\n",
        "\n",
        "        x_ids = X['input_ids'].to(device)\n",
        "        y_ids = Y['input_ids'].to(device)\n",
        "\n",
        "        if len(x_ids.shape) == 3:\n",
        "            x_ids = x_ids.squeeze(1)\n",
        "            y_ids = y_ids.squeeze(1)\n",
        "\n",
        "        if len(x_ids.shape) == 1:\n",
        "            x_ids = x_ids.unsqueeze(0)\n",
        "            y_ids = y_ids.unsqueeze(0)\n",
        "\n",
        "        output = model(x_ids, labels=y_ids)\n",
        "        logits, loss = output['logits'], output['loss']\n",
        "\n",
        "        loss.backward()\n",
        "        for i, module in enumerate(linear_layers):\n",
        "            w_grad = module.weight.grad\n",
        "            if module.bias is not None:\n",
        "                b_grad = module.bias.grad.unsqueeze(-1)\n",
        "                full_grad = torch.cat([w_grad, b_grad], dim=-1)\n",
        "            else:\n",
        "                full_grad = torch.cat(\n",
        "                    [w_grad, torch.zeros([w_grad.shape[0], 1])],\n",
        "                    dim=-1\n",
        "                )\n",
        "            # r = full_grad.view(-1)\n",
        "            # tokenwise_grads = []\n",
        "            # for i, _ in enumerate(x_ids[0]):\n",
        "            #   # print(r[i])\n",
        "            #   tokenwise_grads.append(r[i])\n",
        "            grads[i].append(full_grad)\n",
        "            # print(len(tokenwise_grads))\n",
        "            # token_stacked = torch.cat(tokenwise_grads, dim=-1)\n",
        "            # print(token_stacked.shape)\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "P_4ELGPo-sNR",
        "outputId": "def59f06-7228-47a3-9ea2-2ce635bb8091"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e71de67de6c7>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mz_m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mz_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-e71de67de6c7>\u001b[0m in \u001b[0;36mprocess_prompts\u001b[0;34m(prompts)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprompt_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mz_m_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1674\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2522\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    611\u001b[0m                 )\n\u001b[1;32m    612\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    614\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     ):\n\u001b[0;32m--> 280\u001b[0;31m         return self.attention(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     ):\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward_hook_fn() takes 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "prompt = \"Once upon a time, there was a girl name Alice. Alice enjoyed many things. One of the things Alice enjoyed was playing outside.\"\n",
        "# prompt_enc = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "# out = model.generate(prompt_enc, max_length=300)\n",
        "# print(tokenizer.decode(out[0]))\n",
        "# z_m_ids = torch.nn.functional.pad(out[0], (0, 2048 - len(out[0])), value=50256)\n",
        "\n",
        "# z_m_label = z_m_ids.clone()\n",
        "# z_m_label[:-1] = z_m_ids[1:]\n",
        "# z_m_label[-1] = tokenizer.pad_token_id\n",
        "\n",
        "# z_m = [({'input_ids': z_m_ids}, {'input_ids': z_m_label})]\n",
        "\n",
        "prompts = [\n",
        "    prompt,\n",
        "    \"\"\"Emily and Benjamin were exploring the woods behind their house when they stumbled upon an old, weathered book. It was dusty and had a mysterious aura about it, sparking their curiosity instantly. Benjamin turned to Emily, \"What should we do with the book?\" Emily pondered for a moment, then declared, \"Let's take it to Grandma and Grandpa!\" With that, they dashed off, their hearts pounding with anticipation, eager to unveil their discovery. They burst into their grandparents' home, exclaiming, \"Grandma, Grandpa! Look what we found!\" Their grandparents were taken aback and inquired, \"Where did you find this old book?\"\"\",\n",
        "    \"Once upon a time, there was a small child named Lily. She enjoyed playing with her dolls and observing the butterflies in her garden. Lily had a large, fluffy bunny that she adored immensely. One day, Lily's bunny seemed gloomy. Lily couldn't understand why her bunny was feeling down.\",\n",
        "    \"The sun was beaming down as Peter emerged from his small cottage. He was a naive boy who took great pleasure in frolicking and playing games. He had an old rug that he used as a cape, often pretending to be a knight. Peter always longed for a playmate, but there was no one in sight. With a sigh, he resolved to seek out a companion. As he ventured out of the village, he overheard two voices, one of which was distinctly familiar. It was the voice of his neighbor, elderly Mrs. Smith. Peter crept closer, concealing himself behind a tree and peered through the leaves. Mrs. Smith and a stranger were in conversation, and the stranger appeared to be upset about something. Suddenly, the stranger raised his voice and advanced towards Mrs. Smith.\",\n",
        "    \"\"\"Frank the frog was very troubled. His fountain had broken and he was sure no one could fix it. One day Frank's best friend Sam the snail came to visit him. Sam asked, \"What's wrong Frank?\" Frank said sadly, \"My fountain is broken and I can't fix it.\" But Sam had a plan! He said, \"If you come with me I might know someone who can help you fix your fountain.\" So Frank and Sam went out into the woods, looking for help. And soon they found Snail Bob!\"\"\",\n",
        "\n",
        "]\n",
        "\n",
        "def process_prompts(prompts):\n",
        "  z_m = []\n",
        "  for prompt in prompts:\n",
        "    prompt_enc = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    out = model.generate(prompt_enc, max_length=1000)\n",
        "    print(tokenizer.decode(out[0]))\n",
        "    z_m_ids = torch.nn.functional.pad(out[0], (0, 2048 - len(out[0])), value=50256)\n",
        "    z_m_label = z_m_ids.clone()\n",
        "    z_m_label[:-1] = z_m_ids[1:]\n",
        "    z_m_label[-1] = tokenizer.pad_token_id\n",
        "\n",
        "    z_m.append(({'input_ids': z_m_ids}, {'input_ids': z_m_label}))\n",
        "  return z_m\n",
        "\n",
        "z_m = process_prompts(prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DigTkmCh8Zy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "ihvp = torch.load(f'/content/TinyStories_ihvp_1M.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEc8fZhcjnyV"
      },
      "outputs": [],
      "source": [
        "ihvp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esduEwO3ln_r"
      },
      "outputs": [],
      "source": [
        "train_dataset = TinyStoriesDataset('train', tokenizer, block_size=2048, path='roneneldan/TinyStories')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptj-7VDmldAS"
      },
      "outputs": [],
      "source": [
        "topk = 5\n",
        "\n",
        "all_top_training_samples = []\n",
        "all_top_influences = []\n",
        "all_top_token_influences = []\n",
        "\n",
        "test_dataloader = DataLoader(z_m, batch_size=1)\n",
        "\n",
        "for query, compl in z_m:\n",
        "    # torch.Tensor(list(filter(lambda x: x != 50256, query['input_ids'].tolist()))).long()\n",
        "    print(ihvp.shape)\n",
        "    print(query)\n",
        "    grads = compute_grads(model, [(query, compl)])\n",
        "    print(\"grads[0]\", grads[0][0].view(-1).shape)\n",
        "    print(len(grads))\n",
        "    for l in grads:\n",
        "        print(\"token_query_grad\", l[0].view(-1).shape)\n",
        "    query_grad = torch.cat(\n",
        "        [q[0].view(-1) for q in grads]\n",
        "    )\n",
        "    print(\"query_grad\", query_grad.shape)\n",
        "    top_influences = -1 * torch.einsum(\"ij,j->i\", ihvp, query_grad)\n",
        "    print(\"top_influences\", top_influences.shape)\n",
        "\n",
        "    top_influences, top_samples = torch.topk(top_influences, topk)\n",
        "    print(top_samples)\n",
        "    all_top_training_samples.append(top_samples)\n",
        "    all_top_influences.append(top_influences)\n",
        "    all_top_token_influences.append(1)\n",
        "\n",
        "top_influence_sentences = []\n",
        "for k, (top_samples, top_influences) in enumerate(\n",
        "        zip(all_top_training_samples, all_top_influences)\n",
        "    ):\n",
        "    influence_sentences = []\n",
        "    for s, i in zip(top_samples, top_influences):\n",
        "        s = s.item()\n",
        "        sample = f\"{tokenizer.decode(train_dataset[s][0]['input_ids'])}\"\n",
        "        influence_sentences.append({\"sample\": sample, \"influence\": i.item()})\n",
        "\n",
        "    top_influence_sentences.append({\"samples\": influence_sentences, \"query\": tokenizer.decode(z_m[k][0]['input_ids'])})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F2HolMo1t0p"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/influences.json', 'w') as outfile:\n",
        "    json.dump({\"influences\": top_influence_sentences}, outfile,indent=4)\n",
        "\n",
        "json.dumps({\"influences\": top_influence_sentences})\n",
        "\n",
        "# TODO ensure we are doing this per token when using real data\n",
        "for i, (top_samples, top_influences, top_token_influences) in enumerate(\n",
        "        zip(all_top_training_samples, all_top_influences, all_top_token_influences)\n",
        "    ):\n",
        "        print(f\"Query: {tokenizer.decode(z_m[i][0]['input_ids'])}\")\n",
        "        print(f\"Top {topk} training samples and their influences:\")\n",
        "        for s, i in zip(top_samples, top_influences):\n",
        "            s = s.item()\n",
        "            sample = f\"{tokenizer.decode(train_dataset[s][0]['input_ids'])}\"\n",
        "            print(\n",
        "                f\"###\\n{sample}\\n Influence: {i}\\n\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwz47nwkPuO4"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import einops\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset\n",
        "import string\n",
        "from influence_functions_transformer import influence, InfluenceCalculable\n",
        "from random import sample\n",
        "\n",
        "d_model = 16\n",
        "n_heads = 2\n",
        "d_mlp = 32\n",
        "n_layers = 2\n",
        "vocab_size = 128\n",
        "dataset_length = 200\n",
        "sequence_length = 2048\n",
        "lr = 0.001\n",
        "n_epochs = 5000\n",
        "\n",
        "def dataset_sample(dataset, n_samples):\n",
        "    indices = sample(range(len(dataset)), n_samples)\n",
        "    return [dataset[i] for i in indices]\n",
        "\n",
        "def autoregressive_loss(output, target):\n",
        "    output = einops.rearrange(output, \"b s v -> (b s) v\")\n",
        "    target = einops.rearrange(target, \"b s -> (b s)\")\n",
        "    loss = t.nn.functional.cross_entropy(output, target)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class CharPredictDataset(Dataset):\n",
        "    def __init__(self, length, seq_length):\n",
        "        self.data = self._generate_data(length)\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def _generate_data(self, length):\n",
        "        alphabets = string.ascii_lowercase\n",
        "        numbers = [str(i % 10) for i in range(length // 2)]\n",
        "        return \"\".join(\n",
        "            [alphabets[i % len(alphabets)] + numbers[i] for i in range(length // 2)]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source_seq = self.data[idx : idx + self.seq_length]\n",
        "        return t.tensor([ord(c) for c in source_seq[:-1]], dtype=t.long), t.tensor(\n",
        "            [ord(c) for c in source_seq[1:]], dtype=t.long\n",
        "        )\n",
        "\n",
        "\n",
        "class MultiHeadMaskedAttention(t.nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.q_proj = t.nn.Linear(d_model, d_model)\n",
        "        self.k_proj = t.nn.Linear(d_model, d_model)\n",
        "        self.v_proj = t.nn.Linear(d_model, d_model)\n",
        "        self.out_proj = t.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        Q = einops.rearrange(self.q_proj(X), \"b s (h d) -> b h s d\", h=self.n_heads)\n",
        "        K = einops.rearrange(self.k_proj(X), \"b s (h d) -> b h s d\", h=self.n_heads)\n",
        "        V = einops.rearrange(self.v_proj(X), \"b s (h d) -> b h s d\", h=self.n_heads)\n",
        "\n",
        "        # Compute the scaled dot-product attention\n",
        "        QK = t.einsum(\"b h i d, b h j d -> b h i j\", Q, K)\n",
        "        QK = QK / t.sqrt(t.tensor(self.d_head))\n",
        "        if mask is not None:\n",
        "            QK = QK.masked_fill(mask, -1e9)\n",
        "        QK = t.nn.functional.softmax(QK, dim=-1)\n",
        "\n",
        "        # Compute the output\n",
        "        Y = t.einsum(\"b h i j, b h j d -> b h i d\", QK, V)\n",
        "        Y = einops.rearrange(Y, \"b h s d -> b s (h d)\")\n",
        "\n",
        "        # Apply the output projection\n",
        "        Y = self.out_proj(Y)\n",
        "\n",
        "        return Y\n",
        "\n",
        "\n",
        "class MLPBlock(InfluenceCalculable, t.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.linear = t.nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = t.nn.ReLU()\n",
        "        self.linear2 = t.nn.Linear(hidden_dim, output_dim)\n",
        "        self.input = None\n",
        "\n",
        "        # Save gradient of loss wrt output of linear layer (Ds_l, where s_l = self.linear(a_l_minus_1))\n",
        "        def hook_fn(module, grad_input, grad_output):\n",
        "            self.d_s_l = grad_output[0]\n",
        "\n",
        "        self.linear.register_full_backward_hook(hook_fn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "    def get_a_l_minus_1(self):\n",
        "        # Return the input to the linear layer as a homogenous vector (batch_size, seq_len, input_dim + 1)\n",
        "        print(self.input.shape)\n",
        "        return (\n",
        "            t.cat(\n",
        "                [\n",
        "                    self.input,\n",
        "                    t.ones((self.input.shape[0], self.input.shape[1], 1)).to(\n",
        "                        self.input.device\n",
        "                    ),\n",
        "                ],\n",
        "                dim=-1,\n",
        "            )\n",
        "            .clone()\n",
        "            .detach()\n",
        "        )\n",
        "\n",
        "    def get_d_s_l(self):\n",
        "        # Return the gradient of the loss wrt the output of the linear layer\n",
        "        return self.d_s_l.clone().detach()\n",
        "\n",
        "    def get_dims(self):\n",
        "        # Return the dimensions of the weights - (output_dim, input_dim)\n",
        "        return self.linear.weight.shape\n",
        "\n",
        "    def get_d_w_l(self):\n",
        "        # Return the gradient of the loss wrt the weights\n",
        "        w_grad = self.linear.weight.grad\n",
        "        b_grad = self.linear.bias.grad.unsqueeze(-1)\n",
        "        full_grad = t.cat([w_grad, b_grad], dim=-1)\n",
        "        return full_grad.clone().detach()\n",
        "\n",
        "\n",
        "class TransformerBlock(t.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_mlp):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_mlp = d_mlp\n",
        "        self.attn = MultiHeadMaskedAttention(d_model, n_heads)\n",
        "        self.mlp = MLPBlock(d_model, d_mlp, d_model)\n",
        "        self.layer_norm1 = t.nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = t.nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        attn_output = self.attn(X, mask)\n",
        "        X = self.layer_norm1(X + attn_output)\n",
        "        mlp_output = self.mlp(X)\n",
        "        Y = self.layer_norm2(X + mlp_output)\n",
        "        return Y\n",
        "\n",
        "\n",
        "class DecoderTransformer(t.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_mlp, n_layers, vocab_size, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_mlp = d_mlp\n",
        "        self.n_layers = n_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_input = t.nn.Embedding(vocab_size, d_model)\n",
        "        self.blocks = t.nn.ModuleList(\n",
        "            [TransformerBlock(d_model, n_heads, d_mlp) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.out_proj = t.nn.Linear(d_model, vocab_size)\n",
        "        self.position_embeddings = t.nn.Embedding(max_seq_len, d_model)\n",
        "        self.device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        seq_len = X.size(-1)\n",
        "        mask = t.triu(t.ones(seq_len, seq_len), diagonal=1).bool().to(self.device)\n",
        "        X = self.embed_input(X)\n",
        "        positions = t.arange(0, seq_len, device=X.device).unsqueeze(0)\n",
        "        X = X + self.position_embeddings(positions)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            X = block(X, mask)\n",
        "        Y = self.out_proj(X)\n",
        "        return Y\n",
        "\n",
        "\n",
        "def train_loop(model, data_loader, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print_every = num_epochs // 50\n",
        "    print_every = 1 if print_every == 0 else print_every\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for model_input, target in data_loader:\n",
        "            model_input, target = model_input.to(device), target.to(device)\n",
        "            output = model(model_input)\n",
        "            loss = autoregressive_loss(output, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        if (epoch + 1) % print_every == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def train_char_predict():\n",
        "    small_transformer = DecoderTransformer(\n",
        "        d_model=d_model,\n",
        "        n_heads=n_heads,\n",
        "        d_mlp=d_mlp,\n",
        "        n_layers=n_layers,\n",
        "        vocab_size=vocab_size,\n",
        "        max_seq_len=sequence_length,\n",
        "    )\n",
        "    # dataset = CharPredictDataset(length=dataset_length, seq_length=sequence_length)\n",
        "    dataset = TinyStoriesDataset('train', tokenizer, block_size=2048, path='roneneldan/TinyStories')\n",
        "\n",
        "    data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    optimizer = Adam(small_transformer.parameters(), lr=lr)\n",
        "    train_loop(small_transformer, data_loader, optimizer, num_epochs=n_epochs)\n",
        "    t.save(small_transformer.state_dict(), \"small_transformer.pth\")\n",
        "\n",
        "\n",
        "def calc_influence(model_path):\n",
        "    device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "    # train_dataset = CharPredictDataset(\n",
        "    #     length=dataset_length, seq_length=sequence_length\n",
        "    # )\n",
        "    train_dataset = TinyStoriesDataset('train', tokenizer, block_size=2048, path='roneneldan/TinyStories')\n",
        "    model = DecoderTransformer(\n",
        "        d_model, n_heads, d_mlp, n_layers, vocab_size, sequence_length\n",
        "    )\n",
        "    model.load_state_dict(t.load(model_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    topk = 10\n",
        "    queries = dataset_sample(train_dataset, 5)\n",
        "    gradient_fitting_data = dataset_sample(train_dataset, 100)\n",
        "    search_data = dataset_sample(train_dataset, 100)\n",
        "\n",
        "    all_top_training_samples, all_top_influences = influence(\n",
        "        model,\n",
        "        [b.mlp for b in model.blocks],\n",
        "        queries,\n",
        "        gradient_fitting_data,\n",
        "        search_data,\n",
        "        topk,\n",
        "        device,\n",
        "    )\n",
        "\n",
        "    def decode(token_ids):\n",
        "        try:\n",
        "            return \"\".join([chr(i) for i in token_ids])\n",
        "        except:\n",
        "            return chr(token_ids)\n",
        "\n",
        "    for i, (top_samples, top_influences) in enumerate(\n",
        "        zip(all_top_training_samples, all_top_influences)\n",
        "    ):\n",
        "        print(f\"Query: {decode(queries[i][0])[0]}{decode(queries[i][1])}\")\n",
        "        print(f\"Top {topk} training samples and their influences:\")\n",
        "        for s, i in zip(top_samples, top_influences):\n",
        "            s = s.item()\n",
        "            print(\n",
        "                f\"{decode(search_data[s][0])[0]}{decode(search_data[s][1])} Influence: {i}\"\n",
        "            )\n",
        "\n",
        "\n",
        "def run_model(model_path):\n",
        "    model = DecoderTransformer(\n",
        "        d_model, n_heads, d_mlp, n_layers, vocab_size, sequence_length\n",
        "    )\n",
        "    model.load_state_dict(t.load(model_path))\n",
        "\n",
        "    model.eval()\n",
        "    device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter a string: \")\n",
        "        if user_input == \"exit\":\n",
        "            return\n",
        "        if len(user_input) > (sequence_length - 1):\n",
        "            user_input = user_input[-(sequence_length - 1) :]\n",
        "        token_ids = t.tensor([[ord(c) for c in user_input]], dtype=t.long).to(device)\n",
        "        model_output = model(token_ids)\n",
        "        last_token = model_output[0, -1, :]\n",
        "        topk = t.topk(last_token, 1)\n",
        "        topk_tokens = [chr(int(i)) for i in topk.indices.tolist()]\n",
        "        print(topk_tokens[0])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_char_predict()\n",
        "    # run_model(\"small_transformer.pth\")\n",
        "    # calc_influence(\"/content/small_transformer.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
